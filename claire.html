<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Claire Campbell — LAIDS</title>
  <link rel="stylesheet" href="assets/member.css" />
</head>
<body>
  <!-- Hero banner with image + mauve glass pill nav -->
  <header class="hero hero--short">
    <!-- Top-left pills (homepage + peers) -->
    <nav class="hero-topline">
      <a class="pill mauve" href="index.html">← Homepage</a>
      <a class="pill light" href="christopher.html">Christopher Blignaut's work</a>
      <a class="pill light" href="sian.html">Sian Caine's work</a>
    </nav>
  
    <div class="container hero-grid">
      <!-- LEFT: text -->
      <div class="hero-copy">
        <h1 class="title">Claire Campbell</h1>
        <p class="subtitle">cmpcla004@myuct.ac.za</p>
  
        <!-- primary actions (white-glow pills) -->
        <div class="actions">
          <a class="button glass" href="#resources">Jump to resources</a>
          <a class="button glass" href="#results">See results</a>
        </div>
      </div>
  
      <!-- RIGHT: picture -->
      <figure class="hero-art">
        <img src="images/claire_profile.jpg" alt="Claire Campbell">
      </figure>
    </div>
  </header>

  <main>

    <!-- Abstract -->
    <section id="abstract" class="section">
      <div class="container">
        <article class="card">
          <p class="eyebrow">Abstract</p>
          <h2 class="display">Overview of my final paper</h2>
          <p class="prose">
            My work focused on designing, quantising, and evaluating three compact intrusion-detection models for constrained devices. 
            The goal was to see if they could equal or outperform a baseline model with less compute and memory, and to assess their 
            ability to detect previously unseen attacks.
          </p>
          <p class="prose">
            <em>Note: </em> Convolutional Neural Network (CNN), Convolutional Neural Network Long Short-Term Memory (CNN-LSTM), Deep Belief Network (DBN).
          </p>
          <ul class="checklist light">
            <li><strong>Problem.</strong> Many devices are constrained on compute, memory, and energy, limiting deployment of conventional AI-based intrusion detection.</li>
            <li><strong>Aim.</strong> Test whether lightweight deep models meet device constraints without compromising detection, and assess how post-training quantisation affects that trade-off.</li>
            <li><strong>Models.</strong> Baseline CNN, hybrid CNN–LSTM, and Deep Belief Network (DBN).</li>
            <li><strong>Data & setup.</strong> CICIDS2017 used for training and evaluation.</li>
            <li><strong>Metrics.</strong> Detection (Accuracy, F1) and compute efficiency (latency, throughput, memory footprint).</li>
            <li><strong>Quantisation schemes.</strong> Float16, INT8 Dynamic (weights-only), and full INT8.</li>
          </ul>
        </article>
        
      </div>
    </section>

    <!-- Model designs -->
    <section id="models" class="section">
      <div class="container">
        <h2 class="section-heading">Model Designs</h2>

        <!-- CNN -->
        <figure class="card model-card reverse">
          <h3>Baseline CNN</h3>
          <div class="model-body">
            <img class="model-img" src="images/CNN_arch.png" alt="CNN architecture">
            <figcaption class="model-notes">
              <ul class="clean">
                <li>Two convolutional blocks with batch normalisation and pooling to learn local patterns and progressively reduce features.</li>
                <li>Global Average Pooling minimises overfitting.</li>
                <li>Compact dense head (with optional dropout) maps the pooled features to class logits.</li>
                <li>Softmax layer outputs the final multiclass probabilities.</li>
              </ul>
            </figcaption>
          </div>
        </figure>

        <!-- CNN-LSTM -->
        <figure class="card model-card reverse">
          <h3>CNN-LSTM</h3>
          <div class="model-body">
            <img class="model-img" src="images/CNN-LSTM_arch.png" alt="CNN-LSTM architecture">
            <figcaption class="model-notes">
              <ul class="clean">
                <li>Two TimeDistributed convolutional blocks with pooling and batch normalisation learn local patterns at each timestep.</li>
                <li>LSTM layer captures temporal dynamics across the window. </li>
                <li>Dense head with dropout provides class logits.</li>
                <li>Softmax layer produces the final multiclass output.</li>
              </ul>
            </figcaption>
          </div>
        </figure>

        <!-- DBN -->
        <figure class="card model-card reverse">
          <h3>DBN Architecture</h3>
          <div class="model-body">
            <img class="model-img" src="images/DBN_arch.png" alt="DBN architecture">
            <figcaption class="model-notes">
              <ul class="clean">
                <li>Unsupervised pretraining with stacked Restricted Boltzmann Machines (RBMs) followed by supervised fine-tuning.</li>
                <li>Supervised head: ReLU → Dropout → Dense → Softmax.</li>
                <li>Pretraining learns useful structure without labels, and the supervised head adapts those features to the multiclass classification.</li>
              </ul>
            </figcaption>
          </div>
        </figure>    
      </div>
    </section>
    
    <!-- Experimentation: 3 tiles like your pipeline -->
    <section id="experiments" class="section">
      <div class="container">
        <h2 class="section-heading">Experimentation</h2>
        <div class="exp-tiles">
          <div class="exp-tile">
            <h4>Benchmarking</h4>
            <p style="text-align: justify">
              This experiment establishes a performance baseline by evaluating the full-precision 
              (FP32) CNN–LSTM and DBN under the same protocol as the baseline CNN. 
              Benchmarking enables a fair comparison across architectures and shows how well each model performs 
              and how much computing resources it uses. It also provides the reference point for later 
              checks on whether quantised versions maintain comparable performance 
              while improving efficiency.
            </p>
          </div>
          <div class="exp-tile">
            <h4>Quantisation</h4>
            <p style="text-align: justify;">
              This experiment examines how post-training quantisation affects both accuracy and efficiency. 
              Quantised variants include Float16, INT8 Dynamic, and full INT8 (where supported). These are produced for the CNN, 
              CNN–LSTM, and DBN and compared against their FP32 counterparts. 
              The analysis tests whether smaller, faster models can deliver 
              similar detection quality with lower latency, higher throughput, and reduced memory.
            </p>
          </div>
          <div class="exp-tile">
            <h4>Generalisation</h4>
            <p style="text-align: justify;">
              This experiment assesses the ability of the models and their quantised variants to handle unseen ("zero-day”) attacks. 
              The setup simulates realistic scenarios by withholding entire attack families during training and evaluating detection 
              on traffic that includes those previously unseen threats. This ensures the models learn 
              general patterns of malicious behaviour rather than memorising known signatures. 
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- Results -->
    <section id="results" class="section">
      <div class="container">
        <h2 class="section-heading">Results</h2>
        <div class="result-grid">
          <!-- drop your charts/images here -->
          <figure class="card">
            <img src="images/results_accuracy.png" alt="Accuracy results">
            <figcaption>Multi-class accuracy across models.</figcaption>
          </figure>
          <figure class="card">
            <img src="images/results_quant.png" alt="Quantisation results">
            <figcaption>Storage & latency after quantisation.</figcaption>
          </figure>
        </div>
      </div>
    </section>

    <!-- Resources (Docs) -->
    <section id="resources" class="section">
      <div class="container">
        <h2 class="section-heading">Resources</h2>
        <div class="doc-grid">
          <article class="doc-card">
            <h3>Literature Review</h3>
            <iframe class="doc-preview" src="docs/Literature_Review_Claire_Campbell.pdf#toolbar=0" title="Literature Review"></iframe>
            <div class="doc-actions">
              <a class="button secondary" href="docs/Literature_Review_Claire_Campbell.pdf">Open PDF</a>
            </div>
          </article>

          <article class="doc-card">
            <h3>Final Paper</h3>
            <iframe class="doc-preview" src="docs/Final_Paper_Claire_Campbell.pdf#toolbar=0" title="Final Paper"></iframe>
            <div class="doc-actions">
              <a class="button secondary" href="docs/Final_Paper_Claire_Campbell.pdf">Open PDF</a>
            </div>
          </article>
        </div>
      </div>
    </section>

    <!-- Conclusion -->
    <section id="conclusion" class="section">
      <div class="container">
        <article class="card">
          <p class="eyebrow">Conclusion</p>
          <h2 class="display">Key Takeaways</h2>
          <p class="prose">
            <!-- Your concluding remarks -->
          </p>
        </article>
      </div>
    </section>

  </main>

  <footer class="site-footer">
    <div class="container">© 2025 University of Cape Town — School of IT · LAIDS</div>
  </footer>
</body>
